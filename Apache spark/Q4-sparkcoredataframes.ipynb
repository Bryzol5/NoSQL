{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List, Set\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10000 \"data/sds011-2020-06-28.csv\" > \"data/subset1.csv\"\n",
    "!head -10000 \"data/sds011-2020-06-29.csv\" > \"data/subset2.csv\"\n",
    "!head -10000 \"data/sds011-2020-06-30.csv\" > \"data/subset3.csv\"\n",
    "!cat \"data/subset1.csv\" \"data/subset2.csv\" \"data/subset3.csv\" > \"data/subset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM_LAT = 0.008983112\n",
    "KM_LON = 0.011972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_to_grid_coordinates(lat_lon: Tuple[float]) -> Tuple[int, int]:\n",
    "    lat, lon = lat_lon\n",
    "    x = int(lon // KM_LON)\n",
    "    y = int(lat // KM_LAT)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def possible_neighbour(\n",
    "    p1: Tuple[float, float], p2: Tuple[float, float]\n",
    ") -> List[tuple[float, float]]:\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    if (abs(x1 - x2) == 1 and y1 == y2) or (x1 == x2 and abs(y1 - y2) == 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_tuple(row: Row, field: str = None):\n",
    "    if field is not None:\n",
    "        return row.__getitem__(field).__getitem__(\"_1\"), row.__getitem__(\n",
    "            field\n",
    "        ).__getitem__(\"_2\")\n",
    "    else:\n",
    "        return row.__getitem__(\"_1\"), row.__getitem__(\"_2\")\n",
    "\n",
    "\n",
    "def add_to_set(set: Set[Tuple[int, int]], item: Tuple[int, int]):\n",
    "    set.add(item)\n",
    "    return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(1074, 5321)],), ([(836, 5980), (836, 5981)],), ([(674, 5402)],), ([(727, 5458), (727, 5457)],), ([(687, 5914), (688, 5914)],)]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"q4\").getOrCreate()\n",
    "spark_context = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    data = (\n",
    "        spark_context.textFile(\"data/subset.csv\")\n",
    "        .map(lambda line: line.strip())\n",
    "        .filter(lambda line: len(line) > 1)\n",
    "        .map(lambda line: line.split(\";\"))\n",
    "        .map(lambda all_cols: (all_cols[0], (float(all_cols[2]), float(all_cols[3]))))\n",
    "        .distinct()\n",
    "        .mapValues(deg_to_grid_coordinates)\n",
    "        .map(\n",
    "            lambda id_pos: Row(\n",
    "                id=id_pos[0],\n",
    "                grid_components=id_pos[1],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    grid_sparse = spark.createDataFrame(data)\n",
    "    grid_sparse2 = spark.createDataFrame(data).withColumnRenamed(\n",
    "        \"grid_components\", \"potential_neighbours\"\n",
    "    )\n",
    "    grid_sparse = (\n",
    "        (\n",
    "            grid_sparse.alias(\"a\")\n",
    "            .crossJoin(grid_sparse2.alias(\"b\"))\n",
    "            .drop(grid_sparse2.id)\n",
    "            .where(\"grid_components != potential_neighbours\")\n",
    "            .groupBy([\"id\", \"grid_components\"])\n",
    "            .agg(f.collect_set(\"potential_neighbours\"))\n",
    "        )\n",
    "        .rdd.map(\n",
    "            lambda row: (\n",
    "                row.__getitem__(\"id\"),\n",
    "                (\n",
    "                    get_tuple(row, \"grid_components\"),\n",
    "                    set(\n",
    "                        get_tuple(r)\n",
    "                        for r in row.__getitem__(\"collect_set(potential_neighbours)\")\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        .mapValues(\n",
    "            lambda gc_pn: (\n",
    "                gc_pn[0],\n",
    "                set(pn for pn in gc_pn[1] if possible_neighbour(pn, gc_pn[0])),\n",
    "            )\n",
    "        )\n",
    "        .map(lambda id_gc_n: (list(add_to_set(id_gc_n[1][1], id_gc_n[1][0])),))\n",
    "    )\n",
    "\n",
    "    print(grid_sparse.take(5))\n",
    "    spark_context.stop()\n",
    "finally:\n",
    "    spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spbd",
   "language": "python",
   "name": "spbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec84edcf8b47c1c2ebdfef7f9f9f12c121a387fdba61b4fdd173684298adbc1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
